Big Data Group Project
Benchmark model
Step One: Download the data
Let's import relevant packages:

import numpy as np
import pandas as pd
Import the data:

df = pd.read_excel('C:/Users/ELyons/Group Project/Input.xlsx')
df = df.set_index('date')
df
cgncr	og	rgdpgr	infl	inv	cons	cb	save	eir	u	inact	twhwgr	prodgr
date													
1972-04-01	2.486782	-0.1	6.993290	-1.008889	22.824615	60.879670	1.092325	10.9	5.857047	4.4	24.8	0.506504	3.7
1972-07-01	3.836564	1.2	0.043480	2.482894	21.346813	62.512751	-0.901054	6.4	5.231162	4.3	24.8	0.412324	1.8
1972-10-01	6.604795	3.1	4.362183	5.317455	21.874646	61.490075	0.108275	5.7	5.863047	4.2	24.6	0.593133	2.3
1973-01-01	-2.439896	6.0	0.803017	0.205900	21.670534	58.664693	-2.220407	9.2	5.730127	3.9	24.3	0.986506	7.3
1973-04-01	6.177329	8.3	-0.766982	0.423129	23.725180	61.609384	-0.471239	11.4	5.954723	3.7	24.2	0.404222	5.2
...	...	...	...	...	...	...	...	...	...	...	...	...	...
2020-07-01	13.342683	-0.8	18.960858	-7.121388	15.474804	62.894384	-1.961090	12.4	1.347726	4.8	21.0	8.588667	5.7
2020-10-01	12.736964	-0.8	4.907953	-0.570066	17.195295	61.851000	-5.221878	12.0	0.721711	5.2	21.1	5.588844	1.4
2021-01-01	3.705312	0.4	-4.052871	3.279455	17.768897	57.195700	-1.560736	15.5	1.180677	4.9	21.4	-1.939744	3.0
2021-04-01	12.941423	1.2	4.359814	-2.428987	17.844727	61.846932	-2.080583	13.4	2.379833	4.7	21.1	5.355640	4.9
2021-07-01	4.238560	0.0	1.465925	0.919680	16.695427	63.614550	-4.192257	7.4	2.377453	4.3	21.1	2.526715	-4.5
198 rows × 13 columns

Step Two: Let's look at the data
Target variable
import seaborn as sns
sns.set_theme(style="darkgrid")
from pylab import rcParams
rcParams['figure.figsize'] = 20, 5
​
CGNCR = sns.lineplot(data=df, x = 'date', y="cgncr", color='r').set_title('CGNRC over time, £m - Quarterly')
CGNCR
Text(0.5, 1.0, 'CGNRC over time, £m - Quarterly')

highlevel = sns.pairplot(df)
highlevel
<seaborn.axisgrid.PairGrid at 0x20f09f64c70>

Step three: Let's set a benchmark model
# Define the predictors
X = pd.DataFrame(df['cgncr'])
X['cgncr1'] = df['cgncr'].shift(1)
X['cgncr2'] = df['cgncr'].shift(2)
X['cgncr3'] = df['cgncr'].shift(3)
X['cgncr4'] = df['cgncr'].shift(4)
X = X[['cgncr1','cgncr2','cgncr3','cgncr4']]
X
​
​
cgncr1	cgncr2	cgncr3	cgncr4
date				
1972-04-01	NaN	NaN	NaN	NaN
1972-07-01	2.486782	NaN	NaN	NaN
1972-10-01	3.836564	2.486782	NaN	NaN
1973-01-01	6.604795	3.836564	2.486782	NaN
1973-04-01	-2.439896	6.604795	3.836564	2.486782
...	...	...	...	...
2020-07-01	35.624682	-0.379606	4.334060	1.818269
2020-10-01	13.342683	35.624682	-0.379606	4.334060
2021-01-01	12.736964	13.342683	35.624682	-0.379606
2021-04-01	3.705312	12.736964	13.342683	35.624682
2021-07-01	12.941423	3.705312	12.736964	13.342683
198 rows × 4 columns

# Remove NaN due to introducing the lagged variables\n",
X = X[4:]
X
​
cgncr1	cgncr2	cgncr3	cgncr4
date				
1973-04-01	-2.439896	6.604795	3.836564	2.486782
1973-07-01	6.177329	-2.439896	6.604795	3.836564
1973-10-01	3.231284	6.177329	-2.439896	6.604795
1974-01-01	3.997814	3.231284	6.177329	-2.439896
1974-04-01	-2.979507	3.997814	3.231284	6.177329
...	...	...	...	...
2020-07-01	35.624682	-0.379606	4.334060	1.818269
2020-10-01	13.342683	35.624682	-0.379606	4.334060
2021-01-01	12.736964	13.342683	35.624682	-0.379606
2021-04-01	3.705312	12.736964	13.342683	35.624682
2021-07-01	12.941423	3.705312	12.736964	13.342683
194 rows × 4 columns

#Define the target variable
y = df[['cgncr']]
y
y = y[4:] # make x and y same length
y
cgncr
date	
1973-04-01	6.177329
1973-07-01	3.231284
1973-10-01	3.997814
1974-01-01	-2.979507
1974-04-01	4.902801
...	...
2020-07-01	13.342683
2020-10-01	12.736964
2021-01-01	3.705312
2021-04-01	12.941423
2021-07-01	4.238560
194 rows × 1 columns

Step four: Lets train the model
from sklearn.model_selection import train_test_split 
import statsmodels.api as sm
​
# Note issue with Train Test Split in that it randomises observations in the train and test sections. 
# We need to keep the dates unbroken across train and test
# Test on the last 10 years of the data (Post 2008FC, 2008 Q3 onwards)\n",
​
test_size = 53
​
X_train = X[:-test_size]
X_test = X[-test_size:] 
y_train = y[:-test_size]
y_test = y[-test_size:]
print(X_train)
print(X_test)
print(y_train)
print(y_test)
​
              cgncr1    cgncr2    cgncr3    cgncr4
date                                              
1973-04-01 -2.439896  6.604795  3.836564  2.486782
1973-07-01  6.177329 -2.439896  6.604795  3.836564
1973-10-01  3.231284  6.177329 -2.439896  6.604795
1974-01-01  3.997814  3.231284  6.177329 -2.439896
1974-04-01 -2.979507  3.997814  3.231284  6.177329
...              ...       ...       ...       ...
2007-04-01 -1.934024  3.789846  1.975552  6.460840
2007-07-01  4.653199 -1.934024  3.789846  1.975552
2007-10-01  0.904073  4.653199 -1.934024  3.789846
2008-01-01  5.216170  0.904073  4.653199 -1.934024
2008-04-01 -2.112913  5.216170  0.904073  4.653199

[141 rows x 4 columns]
               cgncr1     cgncr2     cgncr3     cgncr4
date                                                  
2008-07-01   7.770711  -2.112913   5.216170   0.904073
2008-10-01   9.458093   7.770711  -2.112913   5.216170
2009-01-01  16.839697   9.458093   7.770711  -2.112913
2009-04-01   7.262339  16.839697   9.458093   7.770711
2009-07-01  14.096838   7.262339  16.839697   9.458093
2009-10-01   9.544601  14.096838   7.262339  16.839697
2010-01-01  19.673059   9.544601  14.096838   7.262339
2010-04-01   7.521456  19.673059   9.544601  14.096838
2010-07-01  12.614575   7.521456  19.673059   9.544601
2010-10-01   6.656649  12.614575   7.521456  19.673059
2011-01-01  10.604904   6.656649  12.614575   7.521456
2011-04-01   3.200649  10.604904   6.656649  12.614575
2011-07-01   9.774241   3.200649  10.604904   6.656649
2011-10-01   6.415939   9.774241   3.200649  10.604904
2012-01-01   7.204729   6.415939   9.774241   3.200649
2012-04-01   4.755265   7.204729   6.415939   9.774241
2012-07-01   6.350192   4.755265   7.204729   6.415939
2012-10-01   6.073718   6.350192   4.755265   7.204729
2013-01-01   8.689587   6.073718   6.350192   4.755265
2013-04-01   1.070862   8.689587   6.073718   6.350192
2013-07-01   6.501962   1.070862   8.689587   6.073718
2013-10-01   3.208359   6.501962   1.070862   8.689587
2014-01-01   5.445691   3.208359   6.501962   1.070862
2014-04-01   2.188452   5.445691   3.208359   6.501962
2014-07-01   7.479211   2.188452   5.445691   3.208359
2014-10-01   4.348478   7.479211   2.188452   5.445691
2015-01-01   5.129593   4.348478   7.479211   2.188452
2015-04-01   1.043827   5.129593   4.348478   7.479211
2015-07-01   5.894343   1.043827   5.129593   4.348478
2015-10-01   3.733689   5.894343   1.043827   5.129593
2016-01-01   3.607893   3.733689   5.894343   1.043827
2016-04-01  -0.677409   3.607893   3.733689   5.894343
2016-07-01   5.439613  -0.677409   3.607893   3.733689
2016-10-01   5.714828   5.439613  -0.677409   3.607893
2017-01-01   4.776309   5.714828   5.439613  -0.677409
2017-04-01  -2.609338   4.776309   5.714828   5.439613
2017-07-01   2.312982  -2.609338   4.776309   5.714828
2017-10-01   2.484521   2.312982  -2.609338   4.776309
2018-01-01   4.432332   2.484521   2.312982  -2.609338
2018-04-01  -1.892157   4.432332   2.484521   2.312982
2018-07-01   2.622173  -1.892157   4.432332   2.484521
2018-10-01   0.984926   2.622173  -1.892157   4.432332
2019-01-01   3.771878   0.984926   2.622173  -1.892157
2019-04-01  -1.015848   3.771878   0.984926   2.622173
2019-07-01   4.147895  -1.015848   3.771878   0.984926
2019-10-01   1.818269   4.147895  -1.015848   3.771878
2020-01-01   4.334060   1.818269   4.147895  -1.015848
2020-04-01  -0.379606   4.334060   1.818269   4.147895
2020-07-01  35.624682  -0.379606   4.334060   1.818269
2020-10-01  13.342683  35.624682  -0.379606   4.334060
2021-01-01  12.736964  13.342683  35.624682  -0.379606
2021-04-01   3.705312  12.736964  13.342683  35.624682
2021-07-01  12.941423   3.705312  12.736964  13.342683
               cgncr
date                
1973-04-01  6.177329
1973-07-01  3.231284
1973-10-01  3.997814
1974-01-01 -2.979507
1974-04-01  4.902801
...              ...
2007-04-01  4.653199
2007-07-01  0.904073
2007-10-01  5.216170
2008-01-01 -2.112913
2008-04-01  7.770711

[141 rows x 1 columns]
                cgncr
date                 
2008-07-01   9.458093
2008-10-01  16.839697
2009-01-01   7.262339
2009-04-01  14.096838
2009-07-01   9.544601
2009-10-01  19.673059
2010-01-01   7.521456
2010-04-01  12.614575
2010-07-01   6.656649
2010-10-01  10.604904
2011-01-01   3.200649
2011-04-01   9.774241
2011-07-01   6.415939
2011-10-01   7.204729
2012-01-01   4.755265
2012-04-01   6.350192
2012-07-01   6.073718
2012-10-01   8.689587
2013-01-01   1.070862
2013-04-01   6.501962
2013-07-01   3.208359
2013-10-01   5.445691
2014-01-01   2.188452
2014-04-01   7.479211
2014-07-01   4.348478
2014-10-01   5.129593
2015-01-01   1.043827
2015-04-01   5.894343
2015-07-01   3.733689
2015-10-01   3.607893
2016-01-01  -0.677409
2016-04-01   5.439613
2016-07-01   5.714828
2016-10-01   4.776309
2017-01-01  -2.609338
2017-04-01   2.312982
2017-07-01   2.484521
2017-10-01   4.432332
2018-01-01  -1.892157
2018-04-01   2.622173
2018-07-01   0.984926
2018-10-01   3.771878
2019-01-01  -1.015848
2019-04-01   4.147895
2019-07-01   1.818269
2019-10-01   4.334060
2020-01-01  -0.379606
2020-04-01  35.624682
2020-07-01  13.342683
2020-10-01  12.736964
2021-01-01   3.705312
2021-04-01  12.941423
2021-07-01   4.238560
# Linear regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
   
# Define and estimate the model
benchmark = LinearRegression().fit(X_train, y_train)
coef  = benchmark.coef_
constant = benchmark.intercept_
    
# Display the slope coefficients estimates 
# The coefficients
print('Intercept: \\n', benchmark.intercept_)
print('Coefficients: \\n', benchmark.coef_)
​
Intercept: \n [0.40963424]
Coefficients: \n [[ 0.13056651  0.19724578 -0.11665889  0.63306628]]
Step five: test the benchmark model
# Define and estimate the model
​
from sklearn.metrics import r2_score, mean_squared_error
​
ypred_benchmark= benchmark.predict(X_test)
r2_benchmark = round(r2_score(y_test, ypred_benchmark),2)
​
accuracy = {'Benchmark':{'r2':r2_benchmark}} 
accuracy
​
{'Benchmark': {'r2': 0.17}}
MSE_benchmark = round(mean_squared_error(y_test, ypred_benchmark),2)
MSE = {'Benchmark':{'mse': MSE_benchmark}}
MSE
​
{'Benchmark': {'mse': 30.95}}
This would suggest that the benchmark model can only explain 17% of the variation in y over the test period and the MSE appears relatively high. This in part may be because the testing period extends to incl. the pandemic. Let's remove this.

print(X_test)
print(y_test)
​
               cgncr1     cgncr2     cgncr3     cgncr4
date                                                  
2008-07-01   7.770711  -2.112913   5.216170   0.904073
2008-10-01   9.458093   7.770711  -2.112913   5.216170
2009-01-01  16.839697   9.458093   7.770711  -2.112913
2009-04-01   7.262339  16.839697   9.458093   7.770711
2009-07-01  14.096838   7.262339  16.839697   9.458093
2009-10-01   9.544601  14.096838   7.262339  16.839697
2010-01-01  19.673059   9.544601  14.096838   7.262339
2010-04-01   7.521456  19.673059   9.544601  14.096838
2010-07-01  12.614575   7.521456  19.673059   9.544601
2010-10-01   6.656649  12.614575   7.521456  19.673059
2011-01-01  10.604904   6.656649  12.614575   7.521456
2011-04-01   3.200649  10.604904   6.656649  12.614575
2011-07-01   9.774241   3.200649  10.604904   6.656649
2011-10-01   6.415939   9.774241   3.200649  10.604904
2012-01-01   7.204729   6.415939   9.774241   3.200649
2012-04-01   4.755265   7.204729   6.415939   9.774241
2012-07-01   6.350192   4.755265   7.204729   6.415939
2012-10-01   6.073718   6.350192   4.755265   7.204729
2013-01-01   8.689587   6.073718   6.350192   4.755265
2013-04-01   1.070862   8.689587   6.073718   6.350192
2013-07-01   6.501962   1.070862   8.689587   6.073718
2013-10-01   3.208359   6.501962   1.070862   8.689587
2014-01-01   5.445691   3.208359   6.501962   1.070862
2014-04-01   2.188452   5.445691   3.208359   6.501962
2014-07-01   7.479211   2.188452   5.445691   3.208359
2014-10-01   4.348478   7.479211   2.188452   5.445691
2015-01-01   5.129593   4.348478   7.479211   2.188452
2015-04-01   1.043827   5.129593   4.348478   7.479211
2015-07-01   5.894343   1.043827   5.129593   4.348478
2015-10-01   3.733689   5.894343   1.043827   5.129593
2016-01-01   3.607893   3.733689   5.894343   1.043827
2016-04-01  -0.677409   3.607893   3.733689   5.894343
2016-07-01   5.439613  -0.677409   3.607893   3.733689
2016-10-01   5.714828   5.439613  -0.677409   3.607893
2017-01-01   4.776309   5.714828   5.439613  -0.677409
2017-04-01  -2.609338   4.776309   5.714828   5.439613
2017-07-01   2.312982  -2.609338   4.776309   5.714828
2017-10-01   2.484521   2.312982  -2.609338   4.776309
2018-01-01   4.432332   2.484521   2.312982  -2.609338
2018-04-01  -1.892157   4.432332   2.484521   2.312982
2018-07-01   2.622173  -1.892157   4.432332   2.484521
2018-10-01   0.984926   2.622173  -1.892157   4.432332
2019-01-01   3.771878   0.984926   2.622173  -1.892157
2019-04-01  -1.015848   3.771878   0.984926   2.622173
2019-07-01   4.147895  -1.015848   3.771878   0.984926
2019-10-01   1.818269   4.147895  -1.015848   3.771878
2020-01-01   4.334060   1.818269   4.147895  -1.015848
2020-04-01  -0.379606   4.334060   1.818269   4.147895
2020-07-01  35.624682  -0.379606   4.334060   1.818269
2020-10-01  13.342683  35.624682  -0.379606   4.334060
2021-01-01  12.736964  13.342683  35.624682  -0.379606
2021-04-01   3.705312  12.736964  13.342683  35.624682
2021-07-01  12.941423   3.705312  12.736964  13.342683
                cgncr
date                 
2008-07-01   9.458093
2008-10-01  16.839697
2009-01-01   7.262339
2009-04-01  14.096838
2009-07-01   9.544601
2009-10-01  19.673059
2010-01-01   7.521456
2010-04-01  12.614575
2010-07-01   6.656649
2010-10-01  10.604904
2011-01-01   3.200649
2011-04-01   9.774241
2011-07-01   6.415939
2011-10-01   7.204729
2012-01-01   4.755265
2012-04-01   6.350192
2012-07-01   6.073718
2012-10-01   8.689587
2013-01-01   1.070862
2013-04-01   6.501962
2013-07-01   3.208359
2013-10-01   5.445691
2014-01-01   2.188452
2014-04-01   7.479211
2014-07-01   4.348478
2014-10-01   5.129593
2015-01-01   1.043827
2015-04-01   5.894343
2015-07-01   3.733689
2015-10-01   3.607893
2016-01-01  -0.677409
2016-04-01   5.439613
2016-07-01   5.714828
2016-10-01   4.776309
2017-01-01  -2.609338
2017-04-01   2.312982
2017-07-01   2.484521
2017-10-01   4.432332
2018-01-01  -1.892157
2018-04-01   2.622173
2018-07-01   0.984926
2018-10-01   3.771878
2019-01-01  -1.015848
2019-04-01   4.147895
2019-07-01   1.818269
2019-10-01   4.334060
2020-01-01  -0.379606
2020-04-01  35.624682
2020-07-01  13.342683
2020-10-01  12.736964
2021-01-01   3.705312
2021-04-01  12.941423
2021-07-01   4.238560
y_test = y_test[:46]
X_test = X_test[:46]
y_test
X_test
cgncr1	cgncr2	cgncr3	cgncr4
date				
2008-07-01	7.770711	-2.112913	5.216170	0.904073
2008-10-01	9.458093	7.770711	-2.112913	5.216170
2009-01-01	16.839697	9.458093	7.770711	-2.112913
2009-04-01	7.262339	16.839697	9.458093	7.770711
2009-07-01	14.096838	7.262339	16.839697	9.458093
2009-10-01	9.544601	14.096838	7.262339	16.839697
2010-01-01	19.673059	9.544601	14.096838	7.262339
2010-04-01	7.521456	19.673059	9.544601	14.096838
2010-07-01	12.614575	7.521456	19.673059	9.544601
2010-10-01	6.656649	12.614575	7.521456	19.673059
2011-01-01	10.604904	6.656649	12.614575	7.521456
2011-04-01	3.200649	10.604904	6.656649	12.614575
2011-07-01	9.774241	3.200649	10.604904	6.656649
2011-10-01	6.415939	9.774241	3.200649	10.604904
2012-01-01	7.204729	6.415939	9.774241	3.200649
2012-04-01	4.755265	7.204729	6.415939	9.774241
2012-07-01	6.350192	4.755265	7.204729	6.415939
2012-10-01	6.073718	6.350192	4.755265	7.204729
2013-01-01	8.689587	6.073718	6.350192	4.755265
2013-04-01	1.070862	8.689587	6.073718	6.350192
2013-07-01	6.501962	1.070862	8.689587	6.073718
2013-10-01	3.208359	6.501962	1.070862	8.689587
2014-01-01	5.445691	3.208359	6.501962	1.070862
2014-04-01	2.188452	5.445691	3.208359	6.501962
2014-07-01	7.479211	2.188452	5.445691	3.208359
2014-10-01	4.348478	7.479211	2.188452	5.445691
2015-01-01	5.129593	4.348478	7.479211	2.188452
2015-04-01	1.043827	5.129593	4.348478	7.479211
2015-07-01	5.894343	1.043827	5.129593	4.348478
2015-10-01	3.733689	5.894343	1.043827	5.129593
2016-01-01	3.607893	3.733689	5.894343	1.043827
2016-04-01	-0.677409	3.607893	3.733689	5.894343
2016-07-01	5.439613	-0.677409	3.607893	3.733689
2016-10-01	5.714828	5.439613	-0.677409	3.607893
2017-01-01	4.776309	5.714828	5.439613	-0.677409
2017-04-01	-2.609338	4.776309	5.714828	5.439613
2017-07-01	2.312982	-2.609338	4.776309	5.714828
2017-10-01	2.484521	2.312982	-2.609338	4.776309
2018-01-01	4.432332	2.484521	2.312982	-2.609338
2018-04-01	-1.892157	4.432332	2.484521	2.312982
2018-07-01	2.622173	-1.892157	4.432332	2.484521
2018-10-01	0.984926	2.622173	-1.892157	4.432332
2019-01-01	3.771878	0.984926	2.622173	-1.892157
2019-04-01	-1.015848	3.771878	0.984926	2.622173
2019-07-01	4.147895	-1.015848	3.771878	0.984926
2019-10-01	1.818269	4.147895	-1.015848	3.771878
Now, let's re-run test model

ypred_b2= benchmark.predict(X_test)
​
r2_benchmark = round(r2_score(y_test, ypred_b2),2)
accuracy = {'Benchmark':{'r2':r2_benchmark}} 
accuracy
​
{'Benchmark': {'r2': 0.57}}
​
​
MSE_benchmark = round(mean_squared_error(y_test, ypred_b2),2)
MSE = {'Benchmark':{'mse': MSE_benchmark}}
MSE
​
{'Benchmark': {'mse': 8.31}}
We can see here that by removing the impacts of the pandemic, the predictive capabilities of the model increases drastically (17% to 57%) and the MSE drops too.

------------------------------------------------------------------
Main model: Incorporating all predictors from df
list(df)
['cgncr',
 'og',
 'rgdpgr',
 'infl',
 'inv',
 'cons',
 'cb',
 'save',
 'eir',
 'u',
 'inact',
 'twhwgr',
 'prodgr']
Let's lag all predictors:

df['cgncr1'] = df['cgncr'].shift(1)
df['cgncr2'] = df['cgncr'].shift(2)
df['cgncr3'] = df['cgncr'].shift(3)
df['cgncr4'] = df['cgncr'].shift(4)
    
# Output gap. 4 lags
df['og1'] = df['og'].shift(1)
df['og2'] = df['og'].shift(2)
df['og3'] = df['og'].shift(3)
df['og4'] = df['og'].shift(4)
​
# Real GDP growth. 4 lags.
df['rgdpgr1'] = df['rgdpgr'].shift(1)
df['rgdpgr2'] = df['rgdpgr'].shift(2)
df['rgdpgr3'] = df['rgdpgr'].shift(3)
df['rgdpgr4'] = df['rgdpgr'].shift(4)
​
# GDP deflator growth. 6 lags
df['infl1'] = df['infl'].shift(1)
df['infl2'] = df['infl'].shift(2)
df['infl3'] = df['infl'].shift(3)
df['infl4'] = df['infl'].shift(4)
df['infl5'] = df['infl'].shift(5)
df['infl6'] = df['infl'].shift(6)
   
# Investment. 4 lags
df['inv1'] = df['inv'].shift(1)
df['inv2'] = df['inv'].shift(2)
df['inv3'] = df['inv'].shift(3)
df['inv4'] = df['inv'].shift(4)
​
# Consumption. 4 lags
df['cons1'] = df['cons'].shift(1)
df['cons2'] = df['cons'].shift(2)
df['cons3'] = df['cons'].shift(3)
df['cons4'] = df['cons'].shift(4)
  
# Current Balance. 4 lags
df['cb1'] = df['cb'].shift(1)
df['cb2'] = df['cb'].shift(2)
df['cb3'] = df['cb'].shift(3)
df['cb4'] = df['cb'].shift(4)
    
# Savings Ratio. 4 lags
df['save1'] = df['save'].shift(1)
df['save2'] = df['save'].shift(2)
df['save3'] = df['save'].shift(3)
df['save4'] = df['save'].shift(4)
  
# Effective Interest Rate. 4 lags.\n",
df['eir1'] = df['eir'].shift(1)
df['eir2'] = df['eir'].shift(2)
df['eir3'] = df['eir'].shift(3)
df['eir4'] = df['eir'].shift(4)
    
# Unemployment Rate. 4 lags
df['u1'] = df['u'].shift(1)
df['u2'] = df['u'].shift(2)
df['u3'] = df['u'].shift(3)
df['u4'] = df['u'].shift(4)
    
# Inactivity. 4 lags
df['inact1'] = df['inact'].shift(1)
df['inact2'] = df['inact'].shift(2)
df['inact3'] = df['inact'].shift(3)
df['inact4'] = df['inact'].shift(4)
  
# Growth in total working hours worked. 4 lags
df['twhwgr1'] = df['twhwgr'].shift(1)
df['twhwgr2'] = df['twhwgr'].shift(2)
df['twhwgr3'] = df['twhwgr'].shift(3)
df['twhwgr4'] = df['twhwgr'].shift(4)
​
# Productivity growth. 1 lag.\n",
df['prodgr1'] = df['prodgr'].shift(1)
df.columns
Index(['cgncr', 'og', 'rgdpgr', 'infl', 'inv', 'cons', 'cb', 'save', 'eir',
       'u', 'inact', 'twhwgr', 'prodgr', 'cgncr1', 'cgncr2', 'cgncr3',
       'cgncr4', 'og1', 'og2', 'og3', 'og4', 'rgdpgr1', 'rgdpgr2', 'rgdpgr3',
       'rgdpgr4', 'infl1', 'infl2', 'infl3', 'infl4', 'infl5', 'infl6', 'inv1',
       'inv2', 'inv3', 'inv4', 'cons1', 'cons2', 'cons3', 'cons4', 'cb1',
       'cb2', 'cb3', 'cb4', 'save1', 'save2', 'save3', 'save4', 'eir1', 'eir2',
       'eir3', 'eir4', 'u1', 'u2', 'u3', 'u4', 'inact1', 'inact2', 'inact3',
       'inact4', 'twhwgr1', 'twhwgr2', 'twhwgr3', 'twhwgr4', 'prodgr1'],
      dtype='object')
Step two: create a vector of predictors
X = df[['cgncr1', 'cgncr2', 'cgncr3',
       'cgncr4', 'og1', 'og2', 'og3', 'og4', 'rgdpgr1', 'rgdpgr2', 'rgdpgr3',
       'rgdpgr4', 'infl1', 'infl2', 'infl3', 'infl4', 'infl5', 'infl6', 'inv1',
       'inv2', 'inv3', 'inv4', 'cons1', 'cons2', 'cons3', 'cons4', 'cb1',
       'cb2', 'cb3', 'cb4', 'save1', 'save2', 'save3', 'save4', 'eir1', 'eir2',
       'eir3', 'eir4', 'u1', 'u2', 'u3', 'u4', 'inact1', 'inact2', 'inact3',
       'inact4', 'twhwgr1', 'twhwgr2', 'twhwgr3', 'twhwgr4', 'prodgr1']]
​
list(X)
['cgncr1',
 'cgncr2',
 'cgncr3',
 'cgncr4',
 'og1',
 'og2',
 'og3',
 'og4',
 'rgdpgr1',
 'rgdpgr2',
 'rgdpgr3',
 'rgdpgr4',
 'infl1',
 'infl2',
 'infl3',
 'infl4',
 'infl5',
 'infl6',
 'inv1',
 'inv2',
 'inv3',
 'inv4',
 'cons1',
 'cons2',
 'cons3',
 'cons4',
 'cb1',
 'cb2',
 'cb3',
 'cb4',
 'save1',
 'save2',
 'save3',
 'save4',
 'eir1',
 'eir2',
 'eir3',
 'eir4',
 'u1',
 'u2',
 'u3',
 'u4',
 'inact1',
 'inact2',
 'inact3',
 'inact4',
 'twhwgr1',
 'twhwgr2',
 'twhwgr3',
 'twhwgr4',
 'prodgr1']
# As before, remove NaNs from lagged variables.
​
X = X[6:]
X
cgncr1	cgncr2	cgncr3	cgncr4	og1	og2	og3	og4	rgdpgr1	rgdpgr2	...	u4	inact1	inact2	inact3	inact4	twhwgr1	twhwgr2	twhwgr3	twhwgr4	prodgr1
date																					
1973-10-01	3.231284	6.177329	-2.439896	6.604795	7.1	8.3	6.0	3.1	1.576592	-0.766982	...	4.2	24.4	24.2	24.3	24.6	-0.055916	0.404222	0.986506	0.593133	3.9
1974-01-01	3.997814	3.231284	6.177329	-2.439896	6.1	7.1	8.3	6.0	1.803829	1.576592	...	3.9	24.4	24.4	24.2	24.3	0.022379	-0.055916	0.404222	0.986506	2.3
1974-04-01	-2.979507	3.997814	3.231284	6.177329	4.6	6.1	7.1	8.3	-9.868893	1.803829	...	3.7	24.4	24.4	24.4	24.2	-0.257299	0.022379	-0.055916	0.404222	-3.9
1974-07-01	4.902801	-2.979507	3.997814	3.231284	5.6	4.6	6.1	7.1	5.953953	-9.868893	...	3.6	24.3	24.4	24.4	24.4	-0.224316	-0.257299	0.022379	-0.055916	-2.2
1974-10-01	3.478988	4.902801	-2.979507	3.997814	4.1	5.6	4.6	6.1	2.101023	5.953953	...	3.4	24.1	24.3	24.4	24.4	-0.168615	-0.224316	-0.257299	0.022379	-0.1
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
2020-07-01	35.624682	-0.379606	4.334060	1.818269	0.0	0.0	0.0	0.1	-20.232835	-5.424406	...	3.8	20.9	20.4	20.5	20.8	-18.075208	-1.742691	-0.237507	0.000000	-2.1
2020-10-01	13.342683	35.624682	-0.379606	4.334060	-0.8	0.0	0.0	0.0	18.960858	-20.232835	...	3.8	21.0	20.9	20.4	20.5	8.588667	-18.075208	-1.742691	-0.237507	5.7
2021-01-01	12.736964	13.342683	35.624682	-0.379606	-0.8	-0.8	0.0	0.0	4.907953	18.960858	...	4.0	21.1	21.0	20.9	20.4	5.588844	8.588667	-18.075208	-1.742691	1.4
2021-04-01	3.705312	12.736964	13.342683	35.624682	0.4	-0.8	-0.8	0.0	-4.052871	4.907953	...	4.1	21.4	21.1	21.0	20.9	-1.939744	5.588844	8.588667	-18.075208	3.0
2021-07-01	12.941423	3.705312	12.736964	13.342683	1.2	0.4	-0.8	-0.8	4.359814	-4.052871	...	4.8	21.1	21.4	21.1	21.0	5.355640	-1.939744	5.588844	8.588667	4.9
192 rows × 51 columns

Step three: define the predictors and target variable
y = df[['cgncr']]
y = y[6:]
y
​
cgncr
date	
1973-10-01	3.997814
1974-01-01	-2.979507
1974-04-01	4.902801
1974-07-01	3.478988
1974-10-01	8.873561
...	...
2020-07-01	13.342683
2020-10-01	12.736964
2021-01-01	3.705312
2021-04-01	12.941423
2021-07-01	4.238560
192 rows × 1 columns

# Let's remove the effects of the pandemic
​
# Remove pandemic
​
y = y[:185]
X = X[:185]
​
X
cgncr1	cgncr2	cgncr3	cgncr4	og1	og2	og3	og4	rgdpgr1	rgdpgr2	...	u4	inact1	inact2	inact3	inact4	twhwgr1	twhwgr2	twhwgr3	twhwgr4	prodgr1
date																					
1973-10-01	3.231284	6.177329	-2.439896	6.604795	7.1	8.3	6.0	3.1	1.576592	-0.766982	...	4.2	24.4	24.2	24.3	24.6	-0.055916	0.404222	0.986506	0.593133	3.9
1974-01-01	3.997814	3.231284	6.177329	-2.439896	6.1	7.1	8.3	6.0	1.803829	1.576592	...	3.9	24.4	24.4	24.2	24.3	0.022379	-0.055916	0.404222	0.986506	2.3
1974-04-01	-2.979507	3.997814	3.231284	6.177329	4.6	6.1	7.1	8.3	-9.868893	1.803829	...	3.7	24.4	24.4	24.4	24.2	-0.257299	0.022379	-0.055916	0.404222	-3.9
1974-07-01	4.902801	-2.979507	3.997814	3.231284	5.6	4.6	6.1	7.1	5.953953	-9.868893	...	3.6	24.3	24.4	24.4	24.4	-0.224316	-0.257299	0.022379	-0.055916	-2.2
1974-10-01	3.478988	4.902801	-2.979507	3.997814	4.1	5.6	4.6	6.1	2.101023	5.953953	...	3.4	24.1	24.3	24.4	24.4	-0.168615	-0.224316	-0.257299	0.022379	-0.1
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
2018-10-01	0.984926	2.622173	-1.892157	4.432332	0.3	0.2	0.1	0.2	1.601688	1.203009	...	4.4	21.1	21.2	21.1	21.3	0.880418	0.164745	0.486902	0.234261	0.0
2019-01-01	3.771878	0.984926	2.622173	-1.892157	0.2	0.3	0.2	0.1	3.096787	1.601688	...	4.2	20.9	21.1	21.2	21.1	-0.067133	0.880418	0.164745	0.486902	0.5
2019-04-01	-1.015848	3.771878	0.984926	2.622173	0.3	0.2	0.3	0.2	-2.839855	3.096787	...	4.0	20.8	20.9	21.1	21.2	1.046065	-0.067133	0.880418	0.164745	0.5
2019-07-01	4.147895	-1.015848	3.771878	0.984926	0.1	0.3	0.2	0.3	-0.169420	-2.839855	...	4.1	20.7	20.8	20.9	21.1	-0.028493	1.046065	-0.067133	0.880418	0.2
2019-10-01	1.818269	4.147895	-1.015848	3.771878	0.1	0.1	0.3	0.2	1.477213	-0.169420	...	4.0	20.8	20.7	20.8	20.9	0.000000	-0.028493	1.046065	-0.067133	0.7
185 rows × 51 columns

y
cgncr
date	
1973-10-01	3.997814
1974-01-01	-2.979507
1974-04-01	4.902801
1974-07-01	3.478988
1974-10-01	8.873561
...	...
2018-10-01	3.771878
2019-01-01	-1.015848
2019-04-01	4.147895
2019-07-01	1.818269
2019-10-01	4.334060
185 rows × 1 columns

X
cgncr1	cgncr2	cgncr3	cgncr4	og1	og2	og3	og4	rgdpgr1	rgdpgr2	...	u4	inact1	inact2	inact3	inact4	twhwgr1	twhwgr2	twhwgr3	twhwgr4	prodgr1
date																					
1973-10-01	3.231284	6.177329	-2.439896	6.604795	7.1	8.3	6.0	3.1	1.576592	-0.766982	...	4.2	24.4	24.2	24.3	24.6	-0.055916	0.404222	0.986506	0.593133	3.9
1974-01-01	3.997814	3.231284	6.177329	-2.439896	6.1	7.1	8.3	6.0	1.803829	1.576592	...	3.9	24.4	24.4	24.2	24.3	0.022379	-0.055916	0.404222	0.986506	2.3
1974-04-01	-2.979507	3.997814	3.231284	6.177329	4.6	6.1	7.1	8.3	-9.868893	1.803829	...	3.7	24.4	24.4	24.4	24.2	-0.257299	0.022379	-0.055916	0.404222	-3.9
1974-07-01	4.902801	-2.979507	3.997814	3.231284	5.6	4.6	6.1	7.1	5.953953	-9.868893	...	3.6	24.3	24.4	24.4	24.4	-0.224316	-0.257299	0.022379	-0.055916	-2.2
1974-10-01	3.478988	4.902801	-2.979507	3.997814	4.1	5.6	4.6	6.1	2.101023	5.953953	...	3.4	24.1	24.3	24.4	24.4	-0.168615	-0.224316	-0.257299	0.022379	-0.1
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
2018-10-01	0.984926	2.622173	-1.892157	4.432332	0.3	0.2	0.1	0.2	1.601688	1.203009	...	4.4	21.1	21.2	21.1	21.3	0.880418	0.164745	0.486902	0.234261	0.0
2019-01-01	3.771878	0.984926	2.622173	-1.892157	0.2	0.3	0.2	0.1	3.096787	1.601688	...	4.2	20.9	21.1	21.2	21.1	-0.067133	0.880418	0.164745	0.486902	0.5
2019-04-01	-1.015848	3.771878	0.984926	2.622173	0.3	0.2	0.3	0.2	-2.839855	3.096787	...	4.0	20.8	20.9	21.1	21.2	1.046065	-0.067133	0.880418	0.164745	0.5
2019-07-01	4.147895	-1.015848	3.771878	0.984926	0.1	0.3	0.2	0.3	-0.169420	-2.839855	...	4.1	20.7	20.8	20.9	21.1	-0.028493	1.046065	-0.067133	0.880418	0.2
2019-10-01	1.818269	4.147895	-1.015848	3.771878	0.1	0.1	0.3	0.2	1.477213	-0.169420	...	4.0	20.8	20.7	20.8	20.9	0.000000	-0.028493	1.046065	-0.067133	0.7
185 rows × 51 columns

# To ensure data is clean:
print(X.isna().sum())
print(y.isna().sum())
cgncr1     0
cgncr2     0
cgncr3     0
cgncr4     0
og1        0
og2        0
og3        0
og4        0
rgdpgr1    0
rgdpgr2    0
rgdpgr3    0
rgdpgr4    0
infl1      0
infl2      0
infl3      0
infl4      0
infl5      0
infl6      0
inv1       0
inv2       0
inv3       0
inv4       0
cons1      0
cons2      0
cons3      0
cons4      0
cb1        0
cb2        0
cb3        0
cb4        0
save1      0
save2      0
save3      0
save4      0
eir1       0
eir2       0
eir3       0
eir4       0
u1         0
u2         0
u3         0
u4         0
inact1     0
inact2     0
inact3     0
inact4     0
twhwgr1    0
twhwgr2    0
twhwgr3    0
twhwgr4    0
prodgr1    0
dtype: int64
cgncr    0
dtype: int64
Step four: test model accuracy with new lags using training data
# As before, test the final ten years of data post 2008GFC (2008-07-01)
test_size = 46
​
X_train = X[:-test_size]
X_test = X[-test_size:] 
y_train = y[:-test_size]
y_test = y[-test_size:]
​
print(y_test)
print(X_test)
print(y_train)
print(X_train)
                cgncr
date                 
2008-07-01   9.458093
2008-10-01  16.839697
2009-01-01   7.262339
2009-04-01  14.096838
2009-07-01   9.544601
2009-10-01  19.673059
2010-01-01   7.521456
2010-04-01  12.614575
2010-07-01   6.656649
2010-10-01  10.604904
2011-01-01   3.200649
2011-04-01   9.774241
2011-07-01   6.415939
2011-10-01   7.204729
2012-01-01   4.755265
2012-04-01   6.350192
2012-07-01   6.073718
2012-10-01   8.689587
2013-01-01   1.070862
2013-04-01   6.501962
2013-07-01   3.208359
2013-10-01   5.445691
2014-01-01   2.188452
2014-04-01   7.479211
2014-07-01   4.348478
2014-10-01   5.129593
2015-01-01   1.043827
2015-04-01   5.894343
2015-07-01   3.733689
2015-10-01   3.607893
2016-01-01  -0.677409
2016-04-01   5.439613
2016-07-01   5.714828
2016-10-01   4.776309
2017-01-01  -2.609338
2017-04-01   2.312982
2017-07-01   2.484521
2017-10-01   4.432332
2018-01-01  -1.892157
2018-04-01   2.622173
2018-07-01   0.984926
2018-10-01   3.771878
2019-01-01  -1.015848
2019-04-01   4.147895
2019-07-01   1.818269
2019-10-01   4.334060
               cgncr1     cgncr2     cgncr3     cgncr4  og1  og2  og3  og4  \
date                                                                         
2008-07-01   7.770711  -2.112913   5.216170   0.904073  0.7  1.2  1.4  1.7   
2008-10-01   9.458093   7.770711  -2.112913   5.216170 -0.1  0.7  1.2  1.4   
2009-01-01  16.839697   9.458093   7.770711  -2.112913 -2.0 -0.1  0.7  1.2   
2009-04-01   7.262339  16.839697   9.458093   7.770711 -3.7 -2.0 -0.1  0.7   
2009-07-01  14.096838   7.262339  16.839697   9.458093 -4.1 -3.7 -2.0 -0.1   
2009-10-01   9.544601  14.096838   7.262339  16.839697 -3.9 -4.1 -3.7 -2.0   
2010-01-01  19.673059   9.544601  14.096838   7.262339 -3.6 -3.9 -4.1 -3.7   
2010-04-01   7.521456  19.673059   9.544601  14.096838 -2.8 -3.6 -3.9 -4.1   
2010-07-01  12.614575   7.521456  19.673059   9.544601 -2.1 -2.8 -3.6 -3.9   
2010-10-01   6.656649  12.614575   7.521456  19.673059 -1.7 -2.1 -2.8 -3.6   
2011-01-01  10.604904   6.656649  12.614575   7.521456 -1.6 -1.7 -2.1 -2.8   
2011-04-01   3.200649  10.604904   6.656649  12.614575 -1.2 -1.6 -1.7 -2.1   
2011-07-01   9.774241   3.200649  10.604904   6.656649 -1.4 -1.2 -1.6 -1.7   
2011-10-01   6.415939   9.774241   3.200649  10.604904 -1.6 -1.4 -1.2 -1.6   
2012-01-01   7.204729   6.415939   9.774241   3.200649 -1.9 -1.6 -1.4 -1.2   
2012-04-01   4.755265   7.204729   6.415939   9.774241 -1.4 -1.9 -1.6 -1.4   
2012-07-01   6.350192   4.755265   7.204729   6.415939 -1.9 -1.4 -1.9 -1.6   
2012-10-01   6.073718   6.350192   4.755265   7.204729 -1.2 -1.9 -1.4 -1.9   
2013-01-01   8.689587   6.073718   6.350192   4.755265 -1.7 -1.2 -1.9 -1.4   
2013-04-01   1.070862   8.689587   6.073718   6.350192 -1.5 -1.7 -1.2 -1.9   
2013-07-01   6.501962   1.070862   8.689587   6.073718 -1.6 -1.5 -1.7 -1.2   
2013-10-01   3.208359   6.501962   1.070862   8.689587 -1.5 -1.6 -1.5 -1.7   
2014-01-01   5.445691   3.208359   6.501962   1.070862 -1.6 -1.5 -1.6 -1.5   
2014-04-01   2.188452   5.445691   3.208359   6.501962 -1.2 -1.6 -1.5 -1.6   
2014-07-01   7.479211   2.188452   5.445691   3.208359 -0.9 -1.2 -1.6 -1.5   
2014-10-01   4.348478   7.479211   2.188452   5.445691 -0.6 -0.9 -1.2 -1.6   
2015-01-01   5.129593   4.348478   7.479211   2.188452 -0.3 -0.6 -0.9 -1.2   
2015-04-01   1.043827   5.129593   4.348478   7.479211 -0.5 -0.3 -0.6 -0.9   
2015-07-01   5.894343   1.043827   5.129593   4.348478 -0.3 -0.5 -0.3 -0.6   
2015-10-01   3.733689   5.894343   1.043827   5.129593 -0.3 -0.3 -0.5 -0.3   
2016-01-01   3.607893   3.733689   5.894343   1.043827 -0.1 -0.3 -0.3 -0.5   
2016-04-01  -0.677409   3.607893   3.733689   5.894343 -0.6 -0.1 -0.3 -0.3   
2016-07-01   5.439613  -0.677409   3.607893   3.733689 -0.5 -0.6 -0.1 -0.3   
2016-10-01   5.714828   5.439613  -0.677409   3.607893 -0.4 -0.5 -0.6 -0.1   
2017-01-01   4.776309   5.714828   5.439613  -0.677409  0.0 -0.4 -0.5 -0.6   
2017-04-01  -2.609338   4.776309   5.714828   5.439613  0.0  0.0 -0.4 -0.5   
2017-07-01   2.312982  -2.609338   4.776309   5.714828  0.0  0.0  0.0 -0.4   
2017-10-01   2.484521   2.312982  -2.609338   4.776309  0.1  0.0  0.0  0.0   
2018-01-01   4.432332   2.484521   2.312982  -2.609338  0.2  0.1  0.0  0.0   
2018-04-01  -1.892157   4.432332   2.484521   2.312982  0.1  0.2  0.1  0.0   
2018-07-01   2.622173  -1.892157   4.432332   2.484521  0.2  0.1  0.2  0.1   
2018-10-01   0.984926   2.622173  -1.892157   4.432332  0.3  0.2  0.1  0.2   
2019-01-01   3.771878   0.984926   2.622173  -1.892157  0.2  0.3  0.2  0.1   
2019-04-01  -1.015848   3.771878   0.984926   2.622173  0.3  0.2  0.3  0.2   
2019-07-01   4.147895  -1.015848   3.771878   0.984926  0.1  0.3  0.2  0.3   
2019-10-01   1.818269   4.147895  -1.015848   3.771878  0.1  0.1  0.3  0.2   

             rgdpgr1   rgdpgr2  ...   u4  inact1  inact2  inact3  inact4  \
date                            ...                                        
2008-07-01 -2.484286 -0.401344  ...  5.3    22.9    23.0    23.1    23.2   
2008-10-01  0.203798 -2.484286  ...  5.2    23.0    22.9    23.0    23.1   
2009-01-01 -0.982776  0.203798  ...  5.2    22.9    23.0    22.9    23.0   
2009-04-01 -2.890342 -0.982776  ...  5.4    22.8    22.9    23.0    22.9   
2009-07-01 -2.859679 -2.890342  ...  5.9    23.1    22.8    22.9    23.0   
2009-10-01  3.149571 -2.859679  ...  6.4    23.3    23.1    22.8    22.9   
2010-01-01  1.929226  3.149571  ...  7.1    23.4    23.3    23.1    22.8   
2010-04-01 -1.695364  1.929226  ...  7.8    23.6    23.4    23.3    23.1   
2010-07-01 -0.614328 -1.695364  ...  7.8    23.5    23.6    23.4    23.3   
2010-10-01  3.091927 -0.614328  ...  7.8    23.2    23.5    23.6    23.4   
2011-01-01  2.004373  3.091927  ...  8.0    23.5    23.2    23.5    23.6   
2011-04-01 -2.678028  2.004373  ...  7.9    23.4    23.5    23.2    23.5   
2011-07-01 -0.981970 -2.678028  ...  7.8    23.3    23.4    23.5    23.2   
2011-10-01  2.927166 -0.981970  ...  7.9    23.4    23.3    23.4    23.5   
2012-01-01  2.436843  2.927166  ...  7.8    23.2    23.4    23.3    23.4   
2012-04-01 -2.736060  2.436843  ...  7.9    23.1    23.2    23.4    23.3   
2012-07-01 -1.099197 -2.736060  ...  8.3    22.8    23.1    23.2    23.4   
2012-10-01  2.950049 -1.099197  ...  8.4    22.7    22.8    23.1    23.2   
2013-01-01  2.458089  2.950049  ...  8.2    22.4    22.7    22.8    23.1   
2013-04-01 -2.658480  2.458089  ...  8.0    22.6    22.4    22.7    22.8   
2013-07-01  0.495066 -2.658480  ...  7.9    22.5    22.6    22.4    22.7   
2013-10-01  1.147690  0.495066  ...  7.8    22.3    22.5    22.6    22.4   
2014-01-01  2.570902  1.147690  ...  7.8    22.3    22.3    22.5    22.6   
2014-04-01 -1.614777  2.570902  ...  7.7    22.2    22.3    22.3    22.5   
2014-07-01  1.058845 -1.614777  ...  7.6    22.2    22.2    22.3    22.3   
2014-10-01  1.058262  1.058845  ...  7.2    22.2    22.2    22.2    22.3   
2015-01-01  2.669367  1.058262  ...  6.8    22.3    22.2    22.2    22.2   
2015-04-01 -1.988298  2.669367  ...  6.3    22.1    22.3    22.2    22.2   
2015-07-01  0.646893 -1.988298  ...  6.0    22.1    22.1    22.3    22.2   
2015-10-01  1.411420  0.646893  ...  5.7    22.0    22.1    22.1    22.3   
2016-01-01  2.621714  1.411420  ...  5.5    21.8    22.0    22.1    22.1   
2016-04-01 -2.635357  2.621714  ...  5.6    21.8    21.8    22.0    22.1   
2016-07-01  1.622597 -2.635357  ...  5.3    21.6    21.8    21.8    22.0   
2016-10-01  0.216216  1.622597  ...  5.1    21.7    21.6    21.8    21.8   
2017-01-01  3.211839  0.216216  ...  5.1    21.6    21.7    21.6    21.8   
2017-04-01 -2.322865  3.211839  ...  4.9    21.6    21.6    21.7    21.6   
2017-07-01  0.764764 -2.322865  ...  4.8    21.3    21.6    21.6    21.7   
2017-10-01  0.638812  0.764764  ...  4.7    21.6    21.3    21.6    21.6   
2018-01-01  2.818940  0.638812  ...  4.6    21.3    21.6    21.3    21.6   
2018-04-01 -3.363832  2.818940  ...  4.4    21.1    21.3    21.6    21.3   
2018-07-01  1.203009 -3.363832  ...  4.3    21.2    21.1    21.3    21.6   
2018-10-01  1.601688  1.203009  ...  4.4    21.1    21.2    21.1    21.3   
2019-01-01  3.096787  1.601688  ...  4.2    20.9    21.1    21.2    21.1   
2019-04-01 -2.839855  3.096787  ...  4.0    20.8    20.9    21.1    21.2   
2019-07-01 -0.169420 -2.839855  ...  4.1    20.7    20.8    20.9    21.1   
2019-10-01  1.477213 -0.169420  ...  4.0    20.8    20.7    20.8    20.9   

             twhwgr1   twhwgr2   twhwgr3   twhwgr4  prodgr1  
date                                                         
2008-07-01 -1.099361  1.272400  0.000000  0.201870      0.8  
2008-10-01  0.063519 -1.099361  1.272400  0.000000     -0.9  
2009-01-01 -0.603047  0.063519 -1.099361  1.272400     -2.7  
2009-04-01 -1.703034 -0.603047  0.063519 -1.099361     -2.0  
2009-07-01 -0.368165 -1.703034 -0.603047  0.063519     -2.9  
2009-10-01 -0.532551 -0.368165 -1.703034 -0.603047     -0.9  
2010-01-01  0.043706 -0.532551 -0.368165 -1.703034      0.6  
2010-04-01 -0.131062  0.043706 -0.532551 -0.368165      1.6  
2010-07-01  0.863955 -0.131062  0.043706 -0.532551      2.1  
2010-10-01  0.498753  0.863955 -0.131062  0.043706      1.8  
2011-01-01  0.431546  0.498753  0.863955 -0.131062      1.2  
2011-04-01  0.042969  0.431546  0.498753  0.863955      0.6  
2011-07-01 -1.063030  0.042969  0.431546  0.498753      1.5  
2011-10-01  0.607771 -1.063030  0.042969  0.431546      1.1  
2012-01-01 -0.107875  0.607771 -1.063030  0.042969      1.9  
2012-04-01  0.961123 -0.107875  0.607771 -1.063030      1.3  
2012-07-01  0.684565  0.961123 -0.107875  0.607771     -0.7  
2012-10-01  1.009243  0.684565  0.961123 -0.107875     -0.4  
2013-01-01  0.305006  1.009243  0.684565  0.961123     -1.4  
2013-04-01  0.083884  0.305006  1.009243  0.684565     -0.9  
2013-07-01  0.272394  0.083884  0.305006  1.009243      0.3  
2013-10-01  1.180650  0.272394  0.083884  0.305006     -0.3  
2014-01-01  0.175547  1.180650  0.272394  0.083884      0.8  
2014-04-01  0.907123  0.175547  1.180650  0.272394      0.8  
2014-07-01  0.970477  0.907123  0.175547  1.180650     -0.2  
2014-10-01  0.323756  0.970477  0.907123  0.175547      0.5  
2015-01-01  0.594998  0.323756  0.970477  0.907123     -0.2  
2015-04-01  0.280702  0.594998  0.323756  0.970477      0.2  
2015-07-01 -0.129961  0.280702  0.594998  0.323756      1.3  
2015-10-01  0.030030 -0.129961  0.280702  0.594998      1.6  
2016-01-01  1.971380  0.030030 -0.129961  0.280702      0.5  
2016-04-01 -0.500491  1.971380  0.030030 -0.129961      1.0  
2016-07-01  0.256436 -0.500491  1.971380  0.030030      0.6  
2016-10-01  0.236104  0.256436 -0.500491  1.971380      0.4  
2017-01-01  0.392580  0.236104  0.256436 -0.500491      2.0  
2017-04-01  0.625672  0.392580  0.236104  0.256436      1.0  
2017-07-01  0.534344  0.625672  0.392580  0.236104      0.5  
2017-10-01 -0.995361  0.534344  0.625672  0.392580      1.6  
2018-01-01  0.234261 -0.995361  0.534344  0.625672      1.4  
2018-04-01  0.486902  0.234261 -0.995361  0.534344      0.8  
2018-07-01  0.164745  0.486902  0.234261 -0.995361      1.5  
2018-10-01  0.880418  0.164745  0.486902  0.234261      0.0  
2019-01-01 -0.067133  0.880418  0.164745  0.486902      0.5  
2019-04-01  1.046065 -0.067133  0.880418  0.164745      0.5  
2019-07-01 -0.028493  1.046065 -0.067133  0.880418      0.2  
2019-10-01  0.000000 -0.028493  1.046065 -0.067133      0.7  

[46 rows x 51 columns]
               cgncr
date                
1973-10-01  3.997814
1974-01-01 -2.979507
1974-04-01  4.902801
1974-07-01  3.478988
1974-10-01  8.873561
...              ...
2007-04-01  4.653199
2007-07-01  0.904073
2007-10-01  5.216170
2008-01-01 -2.112913
2008-04-01  7.770711

[139 rows x 1 columns]
              cgncr1    cgncr2    cgncr3    cgncr4  og1  og2  og3  og4  \
date                                                                     
1973-10-01  3.231284  6.177329 -2.439896  6.604795  7.1  8.3  6.0  3.1   
1974-01-01  3.997814  3.231284  6.177329 -2.439896  6.1  7.1  8.3  6.0   
1974-04-01 -2.979507  3.997814  3.231284  6.177329  4.6  6.1  7.1  8.3   
1974-07-01  4.902801 -2.979507  3.997814  3.231284  5.6  4.6  6.1  7.1   
1974-10-01  3.478988  4.902801 -2.979507  3.997814  4.1  5.6  4.6  6.1   
...              ...       ...       ...       ...  ...  ...  ...  ...   
2007-04-01 -1.934024  3.789846  1.975552  6.460840  0.9  0.5  0.0  0.1   
2007-07-01  4.653199 -1.934024  3.789846  1.975552  1.7  0.9  0.5  0.0   
2007-10-01  0.904073  4.653199 -1.934024  3.789846  1.7  1.7  0.9  0.5   
2008-01-01  5.216170  0.904073  4.653199 -1.934024  1.4  1.7  1.7  0.9   
2008-04-01 -2.112913  5.216170  0.904073  4.653199  1.2  1.4  1.7  1.7   

             rgdpgr1   rgdpgr2  ...   u4  inact1  inact2  inact3  inact4  \
date                            ...                                        
1973-10-01  1.576592 -0.766982  ...  4.2    24.4    24.2    24.3    24.6   
1974-01-01  1.803829  1.576592  ...  3.9    24.4    24.4    24.2    24.3   
1974-04-01 -9.868893  1.803829  ...  3.7    24.4    24.4    24.4    24.2   
1974-07-01  5.953953 -9.868893  ...  3.6    24.3    24.4    24.4    24.4   
1974-10-01  2.101023  5.953953  ...  3.4    24.1    24.3    24.4    24.4   
...              ...       ...  ...  ...     ...     ...     ...     ...   
2007-04-01  0.161283  0.557341  ...  5.5    23.2    23.0    22.9    22.9   
2007-07-01 -1.665919  0.161283  ...  5.5    23.2    23.2    23.0    22.9   
2007-10-01  3.338862 -1.665919  ...  5.5    23.2    23.2    23.2    23.0   
2008-01-01  1.314069  3.338862  ...  5.5    23.1    23.2    23.2    23.2   
2008-04-01 -0.401344  1.314069  ...  5.4    23.0    23.1    23.2    23.2   

             twhwgr1   twhwgr2   twhwgr3   twhwgr4  prodgr1  
date                                                         
1973-10-01 -0.055916  0.404222  0.986506  0.593133      3.9  
1974-01-01  0.022379 -0.055916  0.404222  0.986506      2.3  
1974-04-01 -0.257299  0.022379 -0.055916  0.404222     -3.9  
1974-07-01 -0.224316 -0.257299  0.022379 -0.055916     -2.2  
1974-10-01 -0.168615 -0.224316 -0.257299  0.022379     -0.1  
...              ...       ...       ...       ...      ...  
2007-04-01  0.160325  0.407813  0.139710 -0.128797      1.4  
2007-07-01  0.437520  0.160325  0.407813  0.139710      1.0  
2007-10-01  0.201870  0.437520  0.160325  0.407813      1.1  
2008-01-01  0.000000  0.201870  0.437520  0.160325      1.6  
2008-04-01  1.272400  0.000000  0.201870  0.437520      0.3  

[139 rows x 51 columns]
# Linear regression
from sklearn.linear_model import LinearRegression
   
# Define and estimate the model
main = LinearRegression().fit(X_train, y_train)
coef  = main.coef_
constant = main.intercept_
    
# Display the slope coefficients estimates 
# The coefficients
print('Intercept: \\n', main.intercept_)
print('Coefficients: \\n', main.coef_)
print('Coefficient of determination: %.2f' % main.score(X_train, y_train))
​
​
Intercept: \n [-5.94428325]
Coefficients: \n [[ 0.27177126  0.16883574 -0.16851411  0.3982681  -0.2168816   0.13471824
  -0.92591791  0.87964423  0.14693494  0.48131734  0.346376    0.51960743
  -0.25988665  0.08643455 -0.19588856  0.117512   -0.06089886 -0.17714469
   0.75799985 -0.24950176 -0.2964379   0.15945893 -0.46242026  0.23099702
  -0.4356286   0.38742826  0.24905859 -0.12927306 -0.23145443 -0.03471061
  -0.11116257  0.07030838 -0.10942982  0.14117797 -0.40644456 -0.07926926
   0.35358715 -0.51590113 -0.4989149  -0.288863    1.65581257 -0.78262785
   0.47566317  0.27969276  1.90007209 -1.69644625 -1.08204209 -0.53284581
  -0.43016207 -0.78075398 -0.49875776]]
Coefficient of determination: 0.77
From the training data, our model with lagged variables achieves an R2 of 0.77.

The R squared shows us what proportion/fraction of variance in y is shared with variance in x. In this case, this model explains 77% of the variation in the training data.

However, the adjusted R2 is only 0.64. The ajusted R2 is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.

As the adj R2 < R2, this would suggest that our training data incl. variables that do not improve the predictive capacity of the model.

Step five: test model on test data
ypred = main.predict(X_test)
print('Coefficient of determination: %.2f' % r2_score(y_test, ypred))
Coefficient of determination: 0.67
ypred_main= main.predict(X_test)
​
r2_main = round(r2_score(y_test, ypred_main),2)
accuracy['Main'] = r2_main
accuracy
​
{'Benchmark': {'r2': 0.57}, 'Main': 0.67}
ypred_main= main.predict(X_test)
​
MSE_main = round(mean_squared_error(y_test, ypred_main),2)
MSE['Main']=MSE_main
MSE
{'Benchmark': {'mse': 8.31}, 'Main': 6.45}
--------------------------------------------------------------------------
Improving the model: Shrinkage
To try and improve the performance of the testing model, we may need to ensure that we are not overfitting our data. To achieve this, we may need to reduce the flexibility of the training model. This can be done by removing less relevant/redundant predictors and the subsequent variance of the training data.

To achieve this, we can use "Shrinkage". This is where we fit a model with all predictors, but reduce some of the coefficients of some predictors to zero or close to zero.

Step one: Ridge Regression
Let's first check that we are continuing to test over the same data (i.e., 1973-10-01 and 2008-07-01 respectively):

y_test
X_test
cgncr1	cgncr2	cgncr3	cgncr4	og1	og2	og3	og4	rgdpgr1	rgdpgr2	...	u4	inact1	inact2	inact3	inact4	twhwgr1	twhwgr2	twhwgr3	twhwgr4	prodgr1
date																					
2008-07-01	7.770711	-2.112913	5.216170	0.904073	0.7	1.2	1.4	1.7	-2.484286	-0.401344	...	5.3	22.9	23.0	23.1	23.2	-1.099361	1.272400	0.000000	0.201870	0.8
2008-10-01	9.458093	7.770711	-2.112913	5.216170	-0.1	0.7	1.2	1.4	0.203798	-2.484286	...	5.2	23.0	22.9	23.0	23.1	0.063519	-1.099361	1.272400	0.000000	-0.9
2009-01-01	16.839697	9.458093	7.770711	-2.112913	-2.0	-0.1	0.7	1.2	-0.982776	0.203798	...	5.2	22.9	23.0	22.9	23.0	-0.603047	0.063519	-1.099361	1.272400	-2.7
2009-04-01	7.262339	16.839697	9.458093	7.770711	-3.7	-2.0	-0.1	0.7	-2.890342	-0.982776	...	5.4	22.8	22.9	23.0	22.9	-1.703034	-0.603047	0.063519	-1.099361	-2.0
2009-07-01	14.096838	7.262339	16.839697	9.458093	-4.1	-3.7	-2.0	-0.1	-2.859679	-2.890342	...	5.9	23.1	22.8	22.9	23.0	-0.368165	-1.703034	-0.603047	0.063519	-2.9
2009-10-01	9.544601	14.096838	7.262339	16.839697	-3.9	-4.1	-3.7	-2.0	3.149571	-2.859679	...	6.4	23.3	23.1	22.8	22.9	-0.532551	-0.368165	-1.703034	-0.603047	-0.9
2010-01-01	19.673059	9.544601	14.096838	7.262339	-3.6	-3.9	-4.1	-3.7	1.929226	3.149571	...	7.1	23.4	23.3	23.1	22.8	0.043706	-0.532551	-0.368165	-1.703034	0.6
2010-04-01	7.521456	19.673059	9.544601	14.096838	-2.8	-3.6	-3.9	-4.1	-1.695364	1.929226	...	7.8	23.6	23.4	23.3	23.1	-0.131062	0.043706	-0.532551	-0.368165	1.6
2010-07-01	12.614575	7.521456	19.673059	9.544601	-2.1	-2.8	-3.6	-3.9	-0.614328	-1.695364	...	7.8	23.5	23.6	23.4	23.3	0.863955	-0.131062	0.043706	-0.532551	2.1
2010-10-01	6.656649	12.614575	7.521456	19.673059	-1.7	-2.1	-2.8	-3.6	3.091927	-0.614328	...	7.8	23.2	23.5	23.6	23.4	0.498753	0.863955	-0.131062	0.043706	1.8
2011-01-01	10.604904	6.656649	12.614575	7.521456	-1.6	-1.7	-2.1	-2.8	2.004373	3.091927	...	8.0	23.5	23.2	23.5	23.6	0.431546	0.498753	0.863955	-0.131062	1.2
2011-04-01	3.200649	10.604904	6.656649	12.614575	-1.2	-1.6	-1.7	-2.1	-2.678028	2.004373	...	7.9	23.4	23.5	23.2	23.5	0.042969	0.431546	0.498753	0.863955	0.6
2011-07-01	9.774241	3.200649	10.604904	6.656649	-1.4	-1.2	-1.6	-1.7	-0.981970	-2.678028	...	7.8	23.3	23.4	23.5	23.2	-1.063030	0.042969	0.431546	0.498753	1.5
2011-10-01	6.415939	9.774241	3.200649	10.604904	-1.6	-1.4	-1.2	-1.6	2.927166	-0.981970	...	7.9	23.4	23.3	23.4	23.5	0.607771	-1.063030	0.042969	0.431546	1.1
2012-01-01	7.204729	6.415939	9.774241	3.200649	-1.9	-1.6	-1.4	-1.2	2.436843	2.927166	...	7.8	23.2	23.4	23.3	23.4	-0.107875	0.607771	-1.063030	0.042969	1.9
2012-04-01	4.755265	7.204729	6.415939	9.774241	-1.4	-1.9	-1.6	-1.4	-2.736060	2.436843	...	7.9	23.1	23.2	23.4	23.3	0.961123	-0.107875	0.607771	-1.063030	1.3
2012-07-01	6.350192	4.755265	7.204729	6.415939	-1.9	-1.4	-1.9	-1.6	-1.099197	-2.736060	...	8.3	22.8	23.1	23.2	23.4	0.684565	0.961123	-0.107875	0.607771	-0.7
2012-10-01	6.073718	6.350192	4.755265	7.204729	-1.2	-1.9	-1.4	-1.9	2.950049	-1.099197	...	8.4	22.7	22.8	23.1	23.2	1.009243	0.684565	0.961123	-0.107875	-0.4
2013-01-01	8.689587	6.073718	6.350192	4.755265	-1.7	-1.2	-1.9	-1.4	2.458089	2.950049	...	8.2	22.4	22.7	22.8	23.1	0.305006	1.009243	0.684565	0.961123	-1.4
2013-04-01	1.070862	8.689587	6.073718	6.350192	-1.5	-1.7	-1.2	-1.9	-2.658480	2.458089	...	8.0	22.6	22.4	22.7	22.8	0.083884	0.305006	1.009243	0.684565	-0.9
2013-07-01	6.501962	1.070862	8.689587	6.073718	-1.6	-1.5	-1.7	-1.2	0.495066	-2.658480	...	7.9	22.5	22.6	22.4	22.7	0.272394	0.083884	0.305006	1.009243	0.3
2013-10-01	3.208359	6.501962	1.070862	8.689587	-1.5	-1.6	-1.5	-1.7	1.147690	0.495066	...	7.8	22.3	22.5	22.6	22.4	1.180650	0.272394	0.083884	0.305006	-0.3
2014-01-01	5.445691	3.208359	6.501962	1.070862	-1.6	-1.5	-1.6	-1.5	2.570902	1.147690	...	7.8	22.3	22.3	22.5	22.6	0.175547	1.180650	0.272394	0.083884	0.8
2014-04-01	2.188452	5.445691	3.208359	6.501962	-1.2	-1.6	-1.5	-1.6	-1.614777	2.570902	...	7.7	22.2	22.3	22.3	22.5	0.907123	0.175547	1.180650	0.272394	0.8
2014-07-01	7.479211	2.188452	5.445691	3.208359	-0.9	-1.2	-1.6	-1.5	1.058845	-1.614777	...	7.6	22.2	22.2	22.3	22.3	0.970477	0.907123	0.175547	1.180650	-0.2
2014-10-01	4.348478	7.479211	2.188452	5.445691	-0.6	-0.9	-1.2	-1.6	1.058262	1.058845	...	7.2	22.2	22.2	22.2	22.3	0.323756	0.970477	0.907123	0.175547	0.5
2015-01-01	5.129593	4.348478	7.479211	2.188452	-0.3	-0.6	-0.9	-1.2	2.669367	1.058262	...	6.8	22.3	22.2	22.2	22.2	0.594998	0.323756	0.970477	0.907123	-0.2
2015-04-01	1.043827	5.129593	4.348478	7.479211	-0.5	-0.3	-0.6	-0.9	-1.988298	2.669367	...	6.3	22.1	22.3	22.2	22.2	0.280702	0.594998	0.323756	0.970477	0.2
2015-07-01	5.894343	1.043827	5.129593	4.348478	-0.3	-0.5	-0.3	-0.6	0.646893	-1.988298	...	6.0	22.1	22.1	22.3	22.2	-0.129961	0.280702	0.594998	0.323756	1.3
2015-10-01	3.733689	5.894343	1.043827	5.129593	-0.3	-0.3	-0.5	-0.3	1.411420	0.646893	...	5.7	22.0	22.1	22.1	22.3	0.030030	-0.129961	0.280702	0.594998	1.6
2016-01-01	3.607893	3.733689	5.894343	1.043827	-0.1	-0.3	-0.3	-0.5	2.621714	1.411420	...	5.5	21.8	22.0	22.1	22.1	1.971380	0.030030	-0.129961	0.280702	0.5
2016-04-01	-0.677409	3.607893	3.733689	5.894343	-0.6	-0.1	-0.3	-0.3	-2.635357	2.621714	...	5.6	21.8	21.8	22.0	22.1	-0.500491	1.971380	0.030030	-0.129961	1.0
2016-07-01	5.439613	-0.677409	3.607893	3.733689	-0.5	-0.6	-0.1	-0.3	1.622597	-2.635357	...	5.3	21.6	21.8	21.8	22.0	0.256436	-0.500491	1.971380	0.030030	0.6
2016-10-01	5.714828	5.439613	-0.677409	3.607893	-0.4	-0.5	-0.6	-0.1	0.216216	1.622597	...	5.1	21.7	21.6	21.8	21.8	0.236104	0.256436	-0.500491	1.971380	0.4
2017-01-01	4.776309	5.714828	5.439613	-0.677409	0.0	-0.4	-0.5	-0.6	3.211839	0.216216	...	5.1	21.6	21.7	21.6	21.8	0.392580	0.236104	0.256436	-0.500491	2.0
2017-04-01	-2.609338	4.776309	5.714828	5.439613	0.0	0.0	-0.4	-0.5	-2.322865	3.211839	...	4.9	21.6	21.6	21.7	21.6	0.625672	0.392580	0.236104	0.256436	1.0
2017-07-01	2.312982	-2.609338	4.776309	5.714828	0.0	0.0	0.0	-0.4	0.764764	-2.322865	...	4.8	21.3	21.6	21.6	21.7	0.534344	0.625672	0.392580	0.236104	0.5
2017-10-01	2.484521	2.312982	-2.609338	4.776309	0.1	0.0	0.0	0.0	0.638812	0.764764	...	4.7	21.6	21.3	21.6	21.6	-0.995361	0.534344	0.625672	0.392580	1.6
2018-01-01	4.432332	2.484521	2.312982	-2.609338	0.2	0.1	0.0	0.0	2.818940	0.638812	...	4.6	21.3	21.6	21.3	21.6	0.234261	-0.995361	0.534344	0.625672	1.4
2018-04-01	-1.892157	4.432332	2.484521	2.312982	0.1	0.2	0.1	0.0	-3.363832	2.818940	...	4.4	21.1	21.3	21.6	21.3	0.486902	0.234261	-0.995361	0.534344	0.8
2018-07-01	2.622173	-1.892157	4.432332	2.484521	0.2	0.1	0.2	0.1	1.203009	-3.363832	...	4.3	21.2	21.1	21.3	21.6	0.164745	0.486902	0.234261	-0.995361	1.5
2018-10-01	0.984926	2.622173	-1.892157	4.432332	0.3	0.2	0.1	0.2	1.601688	1.203009	...	4.4	21.1	21.2	21.1	21.3	0.880418	0.164745	0.486902	0.234261	0.0
2019-01-01	3.771878	0.984926	2.622173	-1.892157	0.2	0.3	0.2	0.1	3.096787	1.601688	...	4.2	20.9	21.1	21.2	21.1	-0.067133	0.880418	0.164745	0.486902	0.5
2019-04-01	-1.015848	3.771878	0.984926	2.622173	0.3	0.2	0.3	0.2	-2.839855	3.096787	...	4.0	20.8	20.9	21.1	21.2	1.046065	-0.067133	0.880418	0.164745	0.5
2019-07-01	4.147895	-1.015848	3.771878	0.984926	0.1	0.3	0.2	0.3	-0.169420	-2.839855	...	4.1	20.7	20.8	20.9	21.1	-0.028493	1.046065	-0.067133	0.880418	0.2
2019-10-01	1.818269	4.147895	-1.015848	3.771878	0.1	0.1	0.3	0.2	1.477213	-0.169420	...	4.0	20.8	20.7	20.8	20.9	0.000000	-0.028493	1.046065	-0.067133	0.7
46 rows × 51 columns

from sklearn.linear_model import Ridge
​
alpha = 10 
ridge = Ridge(alpha = alpha, fit_intercept = True)
ridge.fit(X_train, y_train)
​
coef_ridge = ridge.coef_
constant_ridge = ridge.intercept_
​
ypred_ridge = ridge.predict(X_test)
Note, Ridge shrinkage doesn't fully reduce the coefficient to 0 for predictors. Let's take a look at the initial reduction:

print('Coefficients', coef_ridge)
print('y intercept', constant_ridge)
Coefficients [[ 0.21376651  0.20407487 -0.19106508  0.41022822 -0.11965457 -0.19437316
  -0.30244632  0.38931574 -0.08101622  0.14099674  0.09364305  0.2033601
  -0.17275075  0.02037105 -0.08380022  0.06941216 -0.05363425 -0.1491671
   0.51400836 -0.07540868 -0.24723233  0.21580479 -0.3381722   0.11771004
  -0.19298646  0.21999072  0.2487441  -0.17617881 -0.18997086 -0.05183942
  -0.05232794  0.04247676 -0.10379313  0.13573066 -0.33734647 -0.10887557
   0.12397463 -0.28201334 -0.00435981 -0.02465791  0.05698656  0.02378488
   0.48088627  0.34327827  0.26123304 -0.10384432 -0.36488311 -0.15107292
  -0.31962239 -0.35994586 -0.21202537]]
y intercept [-13.12056103]
Step two: compare the new coefficients with the origianl coefficients from the main model
coef_main = main.coef_
print('Coefficient main model', coef_main)
Coefficient main model [[ 0.27177126  0.16883574 -0.16851411  0.3982681  -0.2168816   0.13471824
  -0.92591791  0.87964423  0.14693494  0.48131734  0.346376    0.51960743
  -0.25988665  0.08643455 -0.19588856  0.117512   -0.06089886 -0.17714469
   0.75799985 -0.24950176 -0.2964379   0.15945893 -0.46242026  0.23099702
  -0.4356286   0.38742826  0.24905859 -0.12927306 -0.23145443 -0.03471061
  -0.11116257  0.07030838 -0.10942982  0.14117797 -0.40644456 -0.07926926
   0.35358715 -0.51590113 -0.4989149  -0.288863    1.65581257 -0.78262785
   0.47566317  0.27969276  1.90007209 -1.69644625 -1.08204209 -0.53284581
  -0.43016207 -0.78075398 -0.49875776]]
df1 = pd.DataFrame(coef_main)
df2 = pd.DataFrame(coef_ridge)
​
df1
0	1	2	3	4	5	6	7	8	9	...	41	42	43	44	45	46	47	48	49	50
0	0.271771	0.168836	-0.168514	0.398268	-0.216882	0.134718	-0.925918	0.879644	0.146935	0.481317	...	-0.782628	0.475663	0.279693	1.900072	-1.696446	-1.082042	-0.532846	-0.430162	-0.780754	-0.498758
1 rows × 51 columns

columns = ['cgncr1', 'cgncr2', 'cgncr3',
       'cgncr4', 'og1', 'og2', 'og3', 'og4', 'rgdpgr1', 'rgdpgr2', 'rgdpgr3',
       'rgdpgr4', 'infl1', 'infl2', 'infl3', 'infl4', 'infl5', 'infl6', 'inv1',
       'inv2', 'inv3', 'inv4', 'cons1', 'cons2', 'cons3', 'cons4', 'cb1',
       'cb2', 'cb3', 'cb4', 'save1', 'save2', 'save3', 'save4', 'eir1', 'eir2',
       'eir3', 'eir4', 'u1', 'u2', 'u3', 'u4', 'inact1', 'inact2', 'inact3',
       'inact4', 'twhwgr1', 'twhwgr2', 'twhwgr3', 'twhwgr4', 'prodgr1']
​
df1.set_axis(columns, axis = 1, inplace = True)
df2.set_axis(columns, axis = 1, inplace = True)
​
comparison_ridge = pd.concat([df1, df2], ignore_index=True)
ourIndex = np.array(['Main', 'Ridge'])
​
comparison_ridge.set_index(ourIndex, inplace=False)
cgncr1	cgncr2	cgncr3	cgncr4	og1	og2	og3	og4	rgdpgr1	rgdpgr2	...	u4	inact1	inact2	inact3	inact4	twhwgr1	twhwgr2	twhwgr3	twhwgr4	prodgr1
Main	0.271771	0.168836	-0.168514	0.398268	-0.216882	0.134718	-0.925918	0.879644	0.146935	0.481317	...	-0.782628	0.475663	0.279693	1.900072	-1.696446	-1.082042	-0.532846	-0.430162	-0.780754	-0.498758
Ridge	0.213767	0.204075	-0.191065	0.410228	-0.119655	-0.194373	-0.302446	0.389316	-0.081016	0.140997	...	0.023785	0.480886	0.343278	0.261233	-0.103844	-0.364883	-0.151073	-0.319622	-0.359946	-0.212025
2 rows × 51 columns

Step three: how does the model perform?
r2_ridge = round(r2_score(y_test, ypred_ridge),2)
​
accuracy['Ridge'] = r2_ridge
accuracy
{'Benchmark': {'r2': 0.57}, 'Main': 0.67, 'Ridge': 0.69}
MSE_ridge = round(mean_squared_error(y_test, ypred_ridge),2)
​
MSE['Ridge'] = MSE_ridge
MSE
{'Benchmark': {'mse': 8.31}, 'Main': 6.45, 'Ridge': 6.13}
Comparing the r2 and MSE, we can see that using Ridge has slightly improved our model.

Step four: Lasso
from sklearn.linear_model import Lasso
​
alpha = 10 # penalty term, lamda
lasso = Lasso(alpha = alpha, fit_intercept = True)
lasso.fit(X_train, y_train)
coef_lasso = lasso.coef_
constant_lasso = lasso.intercept_
​
ypred_lasso = lasso.predict(X_test)
Lasso shrinkage can fully reduce the coefficient to 0 for predictors. As before, let's take a look at the initial reduction:

print('Coefficients', coef_lasso)
print('y intercept', constant_lasso)
Coefficients [ 0.  0.  0.  0. -0. -0. -0. -0. -0. -0.  0.  0.  0.  0.  0.  0.  0.  0.
  0.  0. -0.  0. -0. -0. -0. -0.  0.  0.  0.  0.  0.  0.  0.  0. -0.  0.
 -0. -0.  0.  0. -0. -0.  0.  0.  0.  0. -0. -0. -0. -0. -0.]
y intercept [2.50231087]
Again, let's compare with main:

dfM = pd.DataFrame(coef_main)
dfL = pd.DataFrame(coef_lasso)
dfM
dfL
0
0	0.0
1	0.0
2	0.0
3	0.0
4	-0.0
5	-0.0
6	-0.0
7	-0.0
8	-0.0
9	-0.0
10	0.0
11	0.0
12	0.0
13	0.0
14	0.0
15	0.0
16	0.0
17	0.0
18	0.0
19	0.0
20	-0.0
21	0.0
22	-0.0
23	-0.0
24	-0.0
25	-0.0
26	0.0
27	0.0
28	0.0
29	0.0
30	0.0
31	0.0
32	0.0
33	0.0
34	-0.0
35	0.0
36	-0.0
37	-0.0
38	0.0
39	0.0
40	-0.0
41	-0.0
42	0.0
43	0.0
44	0.0
45	0.0
46	-0.0
47	-0.0
48	-0.0
49	-0.0
50	-0.0
dfL = dfL.transpose()
​
columns = ['cgncr1', 'cgncr2', 'cgncr3',
       'cgncr4', 'og1', 'og2', 'og3', 'og4', 'rgdpgr1', 'rgdpgr2', 'rgdpgr3',
       'rgdpgr4', 'infl1', 'infl2', 'infl3', 'infl4', 'infl5', 'infl6', 'inv1',
       'inv2', 'inv3', 'inv4', 'cons1', 'cons2', 'cons3', 'cons4', 'cb1',
       'cb2', 'cb3', 'cb4', 'save1', 'save2', 'save3', 'save4', 'eir1', 'eir2',
       'eir3', 'eir4', 'u1', 'u2', 'u3', 'u4', 'inact1', 'inact2', 'inact3',
       'inact4', 'twhwgr1', 'twhwgr2', 'twhwgr3', 'twhwgr4', 'prodgr1']
​
dfM.set_axis(columns, axis = 1, inplace = True)
dfL.set_axis(columns, axis = 1, inplace = True)
​
comparison_Lasso = pd.concat([comparison_ridge, dfL], ignore_index=True)
​
ourIndex = np.array(['Main', 'Ridge','Lasso'])
​
comparison_Lasso.set_index(ourIndex, inplace=False)
cgncr1	cgncr2	cgncr3	cgncr4	og1	og2	og3	og4	rgdpgr1	rgdpgr2	...	u4	inact1	inact2	inact3	inact4	twhwgr1	twhwgr2	twhwgr3	twhwgr4	prodgr1
Main	0.271771	0.168836	-0.168514	0.398268	-0.216882	0.134718	-0.925918	0.879644	0.146935	0.481317	...	-0.782628	0.475663	0.279693	1.900072	-1.696446	-1.082042	-0.532846	-0.430162	-0.780754	-0.498758
Ridge	0.213767	0.204075	-0.191065	0.410228	-0.119655	-0.194373	-0.302446	0.389316	-0.081016	0.140997	...	0.023785	0.480886	0.343278	0.261233	-0.103844	-0.364883	-0.151073	-0.319622	-0.359946	-0.212025
Lasso	0.000000	0.000000	0.000000	0.000000	-0.000000	-0.000000	-0.000000	-0.000000	-0.000000	-0.000000	...	-0.000000	0.000000	0.000000	0.000000	0.000000	-0.000000	-0.000000	-0.000000	-0.000000	-0.000000
3 rows × 51 columns

Again, how does the model perform with lasso?

r2_lasso = round(r2_score(y_test, ypred_lasso),2)
accuracy['Lasso'] = r2_lasso
​
accuracy
{'Benchmark': {'r2': 0.57}, 'Main': 0.67, 'Ridge': 0.69, 'Lasso': -0.46}
MSE_lasso = round(mean_squared_error(y_test, ypred_lasso),2)
​
MSE['Lasso'] = MSE_lasso
MSE
{'Benchmark': {'mse': 8.31}, 'Main': 6.45, 'Ridge': 6.13, 'Lasso': 28.53}
More tidily:

comp_pd = pd.DataFrame(accuracy)
comp_pd
​
​
Benchmark	Main	Ridge	Lasso
r2	0.57	0.67	0.69	-0.46
comp_1 = pd.DataFrame(MSE)
comp_1
Benchmark	Main	Ridge	Lasso
mse	8.31	6.45	6.13	28.53
We can see that an alpha of 10 worsens our modle when shrinking with Lasso

Now let's cross validate to find optimal alpha
from sklearn.model_selection import GridSearchCV
​
alpha_space = np.linspace(0.1,100,1000) 
param_grid = {'alpha': alpha_space}
ridge_cv = GridSearchCV(Ridge(), param_grid, cv=5)
ridge_cv.fit(X_train, y_train)
​
ypred_ridge_cv = ridge_cv.predict(X_test)
​
r2_ridge_cv = round(r2_score(y_test, ypred_ridge_cv), 2)
accuracy['CV Ridge'] = r2_ridge_cv
​
MSE_ridge_cv = round(mean_squared_error(y_test, ypred_ridge_cv),2)
MSE['CV Ridge'] = MSE_ridge_cv
Step two: repeat for lasso
alpha_space = np.linspace(0.1,100,1000) 
param_grid = {'alpha': alpha_space}
lasso_cv = GridSearchCV(Lasso(), param_grid, cv=5)
​
lasso_cv.fit(X_train, y_train)
​
ypred_lasso_cv = lasso_cv.predict(X_test)
​
r2_lasso_cv = round(r2_score(y_test, ypred_lasso_cv), 2)
accuracy['CV lasso'] = r2_lasso_cv
​
MSE_lasso_cv = round(mean_squared_error(y_test, ypred_lasso_cv),2)
MSE['CV lasso'] = MSE_lasso_cv
print(ridge_cv.best_estimator_),
print(lasso_cv.best_estimator_)
Ridge(alpha=100.0)
Lasso()
df4 = pd.DataFrame(accuracy)
df5 = pd.DataFrame(MSE)
comparison =pd.concat([df4, df5], ignore_index=True)
comparison
Benchmark	Main	Ridge	Lasso	CV Ridge	CV lasso
0	0.57	0.67	0.69	-0.46	0.66	0.48
1	8.31	6.45	6.13	28.53	6.65	10.08
From this, we can see that cross validation does not improve our model. However, using the initial estimates for Ridge shrinkage does.

-------------------------------------------
Subset selection
This approach involves identifying a subset of k<p predictors which are most relevant for forecasting. To perform best subset selection, we fit a separate least squares regression for each possible combination of the p predictors. We then look at all possible models and identify the best according to some metrics.

The issue is it is very computationally demanding for large models with many predictors, but let's demonstrate with a small number:

Step one: Import packages
import itertools
import time
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn.metrics import mean_squared_error
​
%matplotlib inline 
plt.style.use('ggplot')
Step two: Define general function to run regression using Sklearn and print RSS and R-squared outputs
def fit_linear_reg(X,Y):
    #Fit linear regression model and return RSS and R squared values
    model_k = linear_model.LinearRegression(fit_intercept = True)
    model_k.fit(X,Y)
    RSS = mean_squared_error(Y,model_k.predict(X)) * len(Y)
    R_squared = model_k.score(X,Y)
    return RSS, R_squared
#Importing tqdm for the progress bar
from tqdm import tnrange, tqdm_notebook
​
Y = y_train 
X = X_train[['cgncr1', 'cgncr2', 'eir1', 'eir2', 'inv1', 'inv2']] #this example just uses a small subset of predictors to test the code given the computational power needed to do all
K = len(X.columns)
RSS_list, R_squared_list, feature_list = [],[], []
numb_features = []
​
#Looping over k in 1 to k features in X\n
for k in tnrange(1,len(X.columns) + 1, desc = 'Loop...'):
    #Looping over all possible combinations: from 11 choose k
    for combo in itertools.combinations(X.columns,k):
        tmp_result = fit_linear_reg(X[list(combo)],Y)   #Store temp result 
        RSS_list.append(tmp_result[0])                  #Append lists
        R_squared_list.append(tmp_result[1])
        feature_list.append(combo)
        numb_features.append(len(combo))  
​
#Store in DataFrame
dfdemo = pd.DataFrame({'numb_features': numb_features,'RSS': RSS_list, 'R_squared':R_squared_list,'features':feature_list})
print(dfdemo)
<ipython-input-62-742d384c7fa9>:6: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`
  for k in tnrange(1,len(X.columns) + 1, desc = 'Loop...'):
Loop...:   0%|          | 0/6 [00:00<?, ?it/s]
    numb_features          RSS  R_squared  \
0               1  1448.487013   0.042124   
1               1  1119.560908   0.259641   
2               1  1512.184716   0.000001   
3               1  1512.167189   0.000013   
4               1  1509.285989   0.001918   
..            ...          ...        ...   
58              5  1056.873699   0.301096   
59              5  1060.546767   0.298667   
60              5  1437.716291   0.049247   
61              5  1071.034918   0.291731   
62              6  1056.546178   0.301312   

                                    features  
0                                  (cgncr1,)  
1                                  (cgncr2,)  
2                                    (eir1,)  
3                                    (eir2,)  
4                                    (inv1,)  
..                                       ...  
58        (cgncr1, cgncr2, eir1, inv1, inv2)  
59        (cgncr1, cgncr2, eir2, inv1, inv2)  
60          (cgncr1, eir1, eir2, inv1, inv2)  
61          (cgncr2, eir1, eir2, inv1, inv2)  
62  (cgncr1, cgncr2, eir1, eir2, inv1, inv2)  

[63 rows x 4 columns]
Step three: print data with the 3 lowest RSS and highsest R-squared
​
df_min = dfdemo[dfdemo.groupby('numb_features')['RSS'].transform(min) == dfdemo['RSS']]
df_max = dfdemo[dfdemo.groupby('numb_features')['R_squared'].transform(max) == dfdemo['R_squared']]
display(df_min.head(3))
display(df_max.head(3))
numb_features	RSS	R_squared	features
1	1	1119.560908	0.259641	(cgncr2,)
13	2	1090.167236	0.279079	(cgncr2, inv1)
32	3	1076.136376	0.288357	(cgncr2, eir1, inv1)
numb_features	RSS	R_squared	features
1	1	1119.560908	0.259641	(cgncr2,)
13	2	1090.167236	0.279079	(cgncr2, inv1)
32	3	1076.136376	0.288357	(cgncr2, eir1, inv1)
Step four: add min RSS and max R-squared to datasets for comparison
dfdemo['min_RSS'] = dfdemo.groupby('numb_features')['RSS'].transform(min)
dfdemo['max_R_squared'] = dfdemo.groupby('numb_features')['R_squared'].transform(max)
dfdemo.head()
numb_features	RSS	R_squared	features	min_RSS	max_R_squared
0	1	1448.487013	0.042124	(cgncr1,)	1119.560908	0.259641
1	1	1119.560908	0.259641	(cgncr2,)	1119.560908	0.259641
2	1	1512.184716	0.000001	(eir1,)	1119.560908	0.259641
3	1	1512.167189	0.000013	(eir2,)	1119.560908	0.259641
4	1	1509.285989	0.001918	(inv1,)	1119.560908	0.259641
## Step five: plot results
fig = plt.figure(figsize = (16,6))
ax = fig.add_subplot(1, 2, 1)
​
ax.scatter(dfdemo.numb_features,dfdemo.RSS, alpha = .2, color = 'r' )
ax.set_xlabel('# Features')
ax.set_ylabel('RSS')
ax.set_title('RSS - Best subset selection')
ax.plot(dfdemo.numb_features,dfdemo.min_RSS,color = 'magenta', label = 'Best subset')
ax.legend()
​
ax = fig.add_subplot(1, 2, 2)
ax.scatter(dfdemo.numb_features,dfdemo.R_squared, alpha = .2, color = 'r' )
ax.plot(dfdemo.numb_features,dfdemo.max_R_squared,color = 'magenta', label = 'Best subset')
ax.set_xlabel('# Features')
ax.set_ylabel('R squared')
ax.set_title('R_squared - Best subset selection')
ax.legend()
​
<matplotlib.legend.Legend at 0x10efd705f40>

For computational reasons, the best subset cannot be applied to our model as it would be too onerous to consider combinations for 50 predictors. We have only included as another technical approach that could otherwise be adopted with more time. With 50 predictors, we would need to run: 1 + p(p+1)/2 models 1+50(51)/2 = 1276 models.

Nonetheless, we can see that the model improves when we commbine all of these subset of predictors.

-------------------------------------
Stepwise selection: Forward
Forward Stepwise begins with a model containing no predictors, and then adds predictors to the model, one at the time. At each step, the variable that gives the greatest additional improvement to the fit is added to the model

Step one:Initialise variables
Y = y_train
X = X_train
k = len(X_train.columns)
​
remaining_features = list(X.columns.values)
features = []
RSS_list, R_squared_list = [np.inf], [np.inf] #Due to 1 indexing of the loop...
features_list = dict()
​
for i in range(1,k+1):
    best_RSS = np.inf
    
    for combo in itertools.combinations(remaining_features,1):
​
            RSS = fit_linear_reg(X[list(combo) + features],Y)   #Store temp result 
​
            if RSS[0] < best_RSS:
                best_RSS = RSS[0]
                best_R_squared = RSS[1] 
                best_feature = combo[0]
​
    #Updating variables for next loop
    features.append(best_feature)
    remaining_features.remove(best_feature)
    
    #Saving values for plotting
    RSS_list.append(best_RSS)
    R_squared_list.append(best_R_squared)
    features_list[i] = features.copy()
Step two: Show the first 8 steps
print('Forward stepwise subset selection')
print('Number of features |', 'Features |', 'RSS')
display([(i,features_list[i], round(RSS_list[i])) for i in range(1,9)])
Forward stepwise subset selection
Number of features | Features | RSS
[(1, ['cgncr4'], 724),
 (2, ['cgncr4', 'twhwgr1'], 656),
 (3, ['cgncr4', 'twhwgr1', 'cgncr2'], 629),
 (4, ['cgncr4', 'twhwgr1', 'cgncr2', 'rgdpgr1'], 602),
 (5, ['cgncr4', 'twhwgr1', 'cgncr2', 'rgdpgr1', 'rgdpgr2'], 567),
 (6, ['cgncr4', 'twhwgr1', 'cgncr2', 'rgdpgr1', 'rgdpgr2', 'cb3'], 547),
 (7,
  ['cgncr4', 'twhwgr1', 'cgncr2', 'rgdpgr1', 'rgdpgr2', 'cb3', 'inact1'],
  517),
 (8,
  ['cgncr4',
   'twhwgr1',
   'cgncr2',
   'rgdpgr1',
   'rgdpgr2',
   'cb3',
   'inact1',
   'og2'],
  502)]
From this, we can see the various steps that have been taken to generate the highest r2.

Step three: store in a dataframe and calculate various performance indicators
dfFS = pd.concat([pd.DataFrame({'features':features_list}),pd.DataFrame({'RSS':RSS_list, 'R_squared': R_squared_list})], axis=1, join='inner')
dfFS['numb_features'] = dfFS.index
​
m = len(Y)
p = 11
hat_sigma_squared = (1/(m - p -1)) * min(dfFS['RSS'])
​
​
dfFS['C_p'] = (1/m) * (dfFS['RSS'] + 2 * dfFS['numb_features'] * hat_sigma_squared )
dfFS['AIC'] = (1/(m*hat_sigma_squared)) * (dfFS['RSS'] + 2 * dfFS['numb_features'] * hat_sigma_squared )
dfFS['BIC'] = (1/(m*hat_sigma_squared)) * (dfFS['RSS'] +  np.log(m) * dfFS['numb_features'] * hat_sigma_squared )
dfFS['R_squared_adj'] = 1 - ( (1 - dfFS['R_squared'])*(m-1)/(m-dfFS['numb_features'] -1))
​
Step four: plot metrics
variables = ['C_p', 'AIC','BIC','R_squared_adj']
fig = plt.figure(figsize = (25,6))
​
for i,v in enumerate(variables):
    ax = fig.add_subplot(1, 4, i+1)
    ax.plot(dfFS['numb_features'],dfFS[v], color = 'lightblue')
    ax.scatter(dfFS['numb_features'],dfFS[v], color = 'darkblue')
    if v == ['R_squared_adj']:
        ax.plot(dfFS[v].idxmax(),dfFS[v].max(), marker = 'x', markersize = 20)
    else:
        ax.plot(dfFS[v].idxmin(),dfFS[v].min(), marker = 'x', markersize = 20)
    ax.set_xlabel('Number of predictors')
    ax.set_ylabel(v)
​
fig.suptitle('Subset selection using C_p, AIC, BIC, Adjusted R2', fontsize = 20)
plt.show()

From these graphs, we can see that 10-30 predictors appears to minimise the bias (represented by C_p, AIC and BIC) and maximise the explanatory power of the model (adj r_2).

If we want to breakdown as a table:

dfFS
features	RSS	R_squared	numb_features	C_p	AIC	BIC	R_squared_adj
1	[cgncr4]	724.146404	0.521126	1	5.249141	1.914290	1.935401	0.517631
2	[cgncr4, twhwgr1]	656.294825	0.565996	2	4.800454	1.750660	1.792883	0.559614
3	[cgncr4, twhwgr1, cgncr2]	628.893516	0.584116	3	4.642777	1.693158	1.756492	0.574875
4	[cgncr4, twhwgr1, cgncr2, rgdpgr1]	601.743675	0.602070	4	4.486909	1.636315	1.720760	0.590192
5	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2]	566.763594	0.625203	5	4.274708	1.558928	1.664484	0.611112
6	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb3]	546.511018	0.638595	6	4.168460	1.520181	1.646849	0.622168
7	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	517.262677	0.657937	7	3.997495	1.457832	1.605611	0.639659
8	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	501.626170	0.668278	8	3.924457	1.431196	1.600086	0.647864
9	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	484.900960	0.679338	9	3.843586	1.401703	1.591705	0.656966
10	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	473.630413	0.686791	10	3.801957	1.386522	1.597635	0.662322
11	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	465.429765	0.692214	11	3.782414	1.379395	1.611619	0.665555
12	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	446.479446	0.704746	12	3.685535	1.344065	1.597400	0.676626
13	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	438.728448	0.709871	13	3.669227	1.338117	1.612564	0.679698
14	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	431.602382	0.714584	14	3.657415	1.333809	1.629368	0.682359
15	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	425.644379	0.718524	15	3.654006	1.332566	1.649236	0.684198
16	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	419.079509	0.722865	16	3.646231	1.329731	1.667512	0.686520
17	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	413.500514	0.726555	17	3.645549	1.329482	1.688375	0.688137
18	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	407.751618	0.730356	18	3.643644	1.328787	1.708791	0.689910
19	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	402.487268	0.733838	19	3.645226	1.329364	1.730479	0.691341
20	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	399.204847	0.736008	20	3.661066	1.335141	1.757367	0.691264
21	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	395.673057	0.738344	21	3.675112	1.340263	1.783601	0.691380
22	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	391.784903	0.740915	22	3.686594	1.344450	1.808900	0.691778
23	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	388.900929	0.742822	23	3.705300	1.351272	1.836833	0.691387
24	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	385.744282	0.744910	24	3.722045	1.357379	1.864051	0.691206
25	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	383.036109	0.746700	25	3.742016	1.364662	1.892445	0.690661
26	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	381.246446	0.747884	26	3.768595	1.374355	1.923250	0.689357
27	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	379.502735	0.749037	27	3.795505	1.384169	1.954175	0.687992
28	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	376.027833	0.751335	28	3.809960	1.389440	1.980558	0.688038
29	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	372.507324	0.753663	29	3.824087	1.394592	2.006821	0.688124
30	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	370.098010	0.755256	30	3.846208	1.402660	2.035999	0.687272
31	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	367.833450	0.756754	31	3.869371	1.411107	2.065558	0.686281
32	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	365.091571	0.758567	32	3.889099	1.418302	2.093864	0.685682
33	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	363.282402	0.759763	33	3.915538	1.427943	2.124617	0.684261
34	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	361.562858	0.760901	34	3.942622	1.437820	2.155605	0.682733
35	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	360.082358	0.761880	35	3.971425	1.448325	2.187221	0.680965
36	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	358.560838	0.762886	36	3.999933	1.458721	2.218729	0.679198
37	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	357.270074	0.763739	37	4.030102	1.469723	2.250842	0.677188
38	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	356.109342	0.764507	38	4.061206	1.481066	2.283297	0.675020
39	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	354.992671	0.765245	39	4.092626	1.492525	2.315867	0.672766
40	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	354.235531	0.765746	40	4.126634	1.504927	2.349380	0.670132
41	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	353.202344	0.766429	41	4.158655	1.516605	2.382169	0.667704
42	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	352.443897	0.766931	42	4.192653	1.529004	2.415679	0.664963
43	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	351.848627	0.767325	43	4.227825	1.541830	2.449617	0.662008
44	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	351.070628	0.767839	44	4.261682	1.554178	2.483076	0.659168
45	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	350.184620	0.768425	45	4.294763	1.566241	2.516251	0.656373
46	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	349.285717	0.769019	46	4.327750	1.578272	2.549392	0.653529
47	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	348.771797	0.769359	47	4.363507	1.591312	2.583544	0.650237
48	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	348.541218	0.769512	48	4.401303	1.605095	2.618439	0.646585
49	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	348.365984	0.769628	49	4.439497	1.619024	2.653479	0.642793
50	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	348.307480	0.769666	50	4.478530	1.633259	2.688825	0.638795
51	[cgncr4, twhwgr1, cgncr2, rgdpgr1, rgdpgr2, cb...	348.244431	0.769708	51	4.517531	1.647482	2.724160	0.634709
To find the optimal value:

dfFS[['R_squared_adj', 'R_squared']].max()
R_squared_adj    0.691778
R_squared        0.769708
dtype: float64
dfFS[['R_squared_adj', 'R_squared']].idxmax()
R_squared_adj    22
R_squared        51
dtype: int64
dfFS[['RSS', 'AIC', 'C_p', 'BIC']].min()
RSS    348.244431
AIC      1.328787
C_p      3.643644
BIC      1.591705
dtype: float64
dfFS[['RSS', 'AIC', 'C_p', 'BIC']].idxmin()
RSS    51
AIC    18
C_p    18
BIC     9
dtype: int64
​
